# -*- coding: utf-8 -*-
"""WhisperEvaluation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1u8uKYdc9m30wEn06YW9wHNyjuQCOyB1G

Upload the reference file
"""

from google.colab import files

# Upload the reference file
uploaded = files.upload()

# Get the name of the uploaded file
reference_file_path = next(iter(uploaded))

# Load the reference transcriptions from the .txt file
reference_dict = {}

with open(reference_file_path, 'r', encoding='utf-8') as file:
    for line in file:
        parts = line.strip().split('\t')
        if len(parts) == 2:
            file_name, transcription = parts
            reference_dict[file_name] = transcription

# Now you can use reference_dict for evaluating the metrics
print("Loaded reference transcriptions:", reference_dict)

"""Whisper Model"""

!pip install jiwer
!pip install pystoi

import jiwer

def evaluate_wer_whisper(transcriptions, reference_dict):
    wer_scores = {}

    for file_path, transcription in transcriptions.items():
        reference_text = reference_dict[file_path]
        wer = jiwer.wer(reference_text, transcription)
        wer_scores[file_path] = wer

    return wer_scores

wer_scores = evaluate_wer_whisper(whisper_transcriptions, reference_dict)
print("WER Scores:", wer_scores)

import nltk
from nltk.translate.bleu_score import sentence_bleu

def evaluate_bleu_whisper(transcriptions, reference_dict):
    bleu_scores = {}

    for file_path, transcription in transcriptions.items():
        reference_text = reference_dict[file_path]
        reference_tokens = [reference_text.split()]  # BLEU expects a list of reference tokens
        hypothesis_tokens = transcription.split()

        bleu_score = sentence_bleu(reference_tokens, hypothesis_tokens)
        bleu_scores[file_path] = bleu_score

    return bleu_scores

bleu_scores = evaluate_bleu_whisper(whisper_transcriptions, reference_dict)
print("BLEU Scores:", bleu_scores)

import jiwer

def evaluate_cer_whisper(transcriptions, reference_dict):
    cer_scores = {}

    for file_path, transcription in transcriptions.items():
        reference_text = reference_dict[file_path]
        cer = jiwer.cer(reference_text, transcription)
        cer_scores[file_path] = cer

    return cer_scores

cer_scores = evaluate_cer_whisper(whisper_transcriptions, reference_dict)
print("CER Scores:", cer_scores)

!pip install pesq

from pesq import pesq
from pystoi import stoi

def evaluate_audio_metrics(processed_files, transcriptions):
    pesq_scores = {}
    stoi_scores = {}
    estoi_scores = {}

    for file_path, audio, sr in processed_files:
        # Extract the clean reference audio from the tuple (this assumes the audio is the clean reference)
        ref_signal = audio
        fs_ref = sr

        # PESQ
        pesq_score = pesq(fs_ref, ref_signal, audio, 'wb')
        pesq_scores[file_path] = pesq_score

        # STOI and eSTOI
        stoi_score = stoi(ref_signal, audio, sr, extended=False)
        estoi_score = stoi(ref_signal, audio, sr, extended=True)
        stoi_scores[file_path] = stoi_score
        estoi_scores[file_path] = estoi_score

    return pesq_scores, stoi_scores, estoi_scores

pesq_scores, stoi_scores, estoi_scores = evaluate_audio_metrics(processed_files, whisper_transcriptions)

print("PESQ Scores:", pesq_scores)
print("STOI Scores:", stoi_scores)
print("eSTOI Scores:", estoi_scores)

import pandas as pd
results_df = pd.DataFrame({
    'file_path': list(wer_scores.keys()),
    'WER': list(wer_scores.values()),
    'PESQ': list(pesq_scores.values()),
    'STOI': list(stoi_scores.values()),
    'eSTOI': list(estoi_scores.values()),
})

print(results_df)

"""Speed Test and resource Usage"""

import time

def transcribe_with_timing(processed_files):
    transcriptions = {}
    processing_times = {}

    for file_path, audio, sr in processed_files:
        start_time = time.time()

        # Preprocess audio
        inputs = Whisper_processor(audio, sampling_rate=sr, return_tensors="pt")
        audio_tensor = inputs.input_features

        # Ensure the model is in evaluation mode
        Whisper_model.eval()

        # Generate transcription
        with torch.no_grad():
            output = Whisper_model.generate(audio_tensor)

        # Decode the transcription
        transcription = Whisper_processor.decode(output[0], skip_special_tokens=True)
        transcriptions[file_path] = transcription

        # Measure the processing time
        processing_time = time.time() - start_time
        processing_times[file_path] = processing_time

        print(f"Transcribed {file_path} in {processing_time:.2f} seconds")
        print(f"Transcription: {transcription}")
        print("---")

    return transcriptions, processing_times

whisper_transcriptions, processing_times = transcribe_with_timing(processed_files)

# Print overall processing times
print("Processing times for each file:")
for file_path, time_taken in processing_times.items():
    print(f"{file_path}: {time_taken:.2f} seconds")

import psutil
import torch

def monitor_resources(processed_files):
    transcriptions = {}
    resource_usage = {}

    for file_path, audio, sr in processed_files:
        start_cpu = psutil.cpu_percent(interval=None)
        start_mem = psutil.virtual_memory().used / (1024 ** 3)  # Convert to GB
        start_gpu_mem = torch.cuda.memory_allocated() / (1024 ** 3)  # Convert to GB

        start_time = time.time()

        # Preprocess audio
        inputs = Whisper_processor(audio, sampling_rate=sr, return_tensors="pt")
        audio_tensor = inputs.input_features

        # Ensure the model is in evaluation mode
        Whisper_model.eval()

        # Generate transcription
        with torch.no_grad():
            output = Whisper_model.generate(audio_tensor)

        # Decode the transcription
        transcription = Whisper_processor.decode(output[0], skip_special_tokens=True)
        transcriptions[file_path] = transcription

        # Measure the processing time
        processing_time = time.time() - start_time

        # Capture resource usage
        end_cpu = psutil.cpu_percent(interval=None)
        end_mem = psutil.virtual_memory().used / (1024 ** 3)  # Convert to GB
        end_gpu_mem = torch.cuda.memory_allocated() / (1024 ** 3)  # Convert to GB

        resource_usage[file_path] = {
            "CPU (%)": end_cpu - start_cpu,
            "Memory (GB)": end_mem - start_mem,
            "GPU Memory (GB)": end_gpu_mem - start_gpu_mem,
            "Processing Time (s)": processing_time
        }

        print(f"Transcribed {file_path} in {processing_time:.2f} seconds")
        print(f"Resource Usage: CPU {end_cpu - start_cpu}%, Memory {end_mem - start_mem:.2f} GB, GPU Memory {end_gpu_mem - start_gpu_mem:.2f} GB")
        print("---")

    return transcriptions, resource_usage

whisper_transcriptions, resource_usage = monitor_resources(processed_files)

# Print resource usage summary
print("Resource usage summary for each file:")
for file_path, usage in resource_usage.items():
    print(f"{file_path}: {usage}")

