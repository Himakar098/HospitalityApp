# -*- coding: utf-8 -*-
"""STT Model Test2 Code_suman.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13urD9lnahYFQGAVOSXDKAYGeGDbdis5-

# **Step 1: Data Preprocessing**

Converting the Audio files into WAV format andand resample to 16kHz. Converting to mono if they are in stereo format.Using librosa to load and resample audio.

Necessary imports
"""

!pip install transformers datasets torch

import librosa
import soundfile as sf
import os
import numpy as np
from IPython.display import Audio, display

"""## Load files

### Load audio files
"""

def load_audio(file_paths):
    loaded_files = []

    for file_path in file_paths:
        # Load audio file
        audio, sr = librosa.load(file_path, sr=None, mono=False)
        loaded_files.append((file_path, audio, sr))

        print(f"Loaded {file_path}")
        print(f"Shape: {audio.shape}, Sample rate: {sr}")
        print("---")

    return loaded_files

"""**usage**

You can also directly upload the files usind this
"""

from google.colab import files

uploaded = files.upload()  # This will prompt you to upload files

file_paths = list(uploaded.keys())

# Load audio files
loaded_files = load_audio(file_paths)

"""Data **Preprocessing**"""

def preprocess_audio(loaded_files, target_sr=16000, mono=True):
    """
    Preprocess loaded audio files and display them for listening.

    :param loaded_files: list of tuples (file_path, audio_array, sample_rate)
    :param target_sr: int, target sample rate (default: 16000)
    :param mono: bool, whether to convert to mono (default: True)
    :return: list of tuples (original_path, processed_audio_array, sr)
    """
    processed_files = []

    for file_path, audio, sr in loaded_files:
        original_filename = os.path.splitext(os.path.basename(file_path))[0]

        # Display original audio
        print(f"Original audio: {original_filename}")
        display(Audio(audio, rate=sr))

        # Resample if necessary
        if sr != target_sr:
            audio = librosa.resample(y=audio, orig_sr=sr, target_sr=target_sr)

        # Convert to mono if required
        if mono and audio.ndim > 1:
            audio = librosa.to_mono(audio)

        processed_files.append((file_path, audio, target_sr))

        # Display processed audio
        print(f"Processed audio: {original_filename}")
        display(Audio(audio, rate=target_sr))

        print(f"Processed {file_path}")
        print(f"Shape: {audio.shape}, Sample rate: {target_sr}")
        print("---")

    return processed_files

# Process loaded audio files
processed_files = preprocess_audio(loaded_files, target_sr=16000, mono=True)

"""# **Data Augmentation Techniques**

### Add background noise

Speed perturbation

Pitch shifting

SpecAugment (time and frequency masking)

Combining augmentations -- do not run this part. Need to be updated.

Room impulse response simulation

# Testing Models
"""

from huggingface_hub import notebook_login

notebook_login()

"""### 1. **Whisper**:

Data preprocessing:

a. Audio files should be in WAV format, 16-bit PCM.

b. Use librosa or pydub to load and resample audio to 16kHz if necessary.

Data augmentation:

a. Add background noise

b. Apply speed perturbation

c. Simulate room acoustics

"""

from transformers import WhisperForConditionalGeneration, WhisperProcessor
import torch

# Load Whisper model and processor
Whisper_processor = WhisperProcessor.from_pretrained("openai/whisper-medium")
Whisper_model = WhisperForConditionalGeneration.from_pretrained("openai/whisper-medium")

def transcribe_audio_files(processed_files):

    transcriptions = {}

    for file_path, audio, sr in processed_files:
        # Preprocess audio
        inputs = Whisper_processor(audio, sampling_rate=sr, return_tensors="pt")
        audio_tensor = inputs.input_features  # Use 'input_features' instead of 'input_values'

        # Ensure the model is in evaluation mode
        Whisper_model.eval()

        # Generate transcription
        with torch.no_grad():
            output = Whisper_model.generate(audio_tensor)

        # Decode the transcription
        transcription = Whisper_processor.decode(output[0], skip_special_tokens=True)
        transcriptions[file_path] = transcription

        print(f"Transcribed {file_path}")
        print(f"Transcription: {transcription}")
        print("---")

    return transcriptions

processed_files

whisper_transcriptions = transcribe_audio_files(processed_files)

import json
import time

def save_transcriptions(transcriptions, filename):
      # Create the directory if it doesn't exist
    directory = os.path.dirname(filename)
    if not os.path.exists(directory):
        os.makedirs(directory)

    with open(filename, 'w') as f:
        json.dump(transcriptions, f, indent=4)

# Generate a unique filename based on the current time
timestamp = time.strftime("%Y%m%d-%H%M%S")
filename = f"/content/drive/MyDrive/STT/whisper_transcriptions_{timestamp}.json"

# Save to a file
save_transcriptions(whisper_transcriptions, filename)

!ls /content/drive/MyDrive/STT

from google.colab import drive

# Check if the directory is empty
import os
if not os.path.exists('/content/mydrive') or not os.listdir('/content/mydrive'):
  drive.mount('/content/mydrive')
else:
  print("The directory '/content/mydrive' is not empty. Please clear it or choose a different mount point.")

"""### **2. Wav2vec2**

### 3. Wav2vec2-BERT
"""

from transformers import AutoProcessor, Wav2Vec2BertForCTC
import torch
import numpy as np

# Initialize the processor and model
wav2vec2bert_processor = AutoProcessor.from_pretrained("hf-audio/wav2vec2-bert-CV16-en")
wav2vec2bert_model = Wav2Vec2BertForCTC.from_pretrained("hf-audio/wav2vec2-bert-CV16-en")

def transcribe_with_wav2vec2bert(processed_files):
    transcriptions = {}
    for file_path, audio, sr in processed_files:
        if sr != 16000:
            continue  # Wav2Vec2 requires 16kHz
        inputs = wav2vec2bert_processor(audio, sampling_rate=sr, return_tensors="pt")
        with torch.no_grad():
            logits = wav2vec2bert_model(inputs.input_features).logits
            predicted_ids = torch.argmax(logits, dim=-1)
        transcription = wav2vec2bert_processor.batch_decode(predicted_ids)[0]
        transcriptions[file_path] = transcription
        print(f"Transcribed {file_path}")
        print(f"Transcription: {transcription}")
        print("---")
    return transcriptions

wav2vec2bert_transcriptions = transcribe_with_wav2vec2bert(processed_files)

import json
import time

def save_transcriptions(transcriptions, filename):
      # Create the directory if it doesn't exist
    directory = os.path.dirname(filename)
    if not os.path.exists(directory):
        os.makedirs(directory)

    with open(filename, 'w') as f:
        json.dump(transcriptions, f, indent=4)

# Generate a unique filename based on the current time
timestamp = time.strftime("%Y%m%d-%H%M%S")
filename = f"/content/drive/MyDrive/STT/wav2vec2bert_transcriptions{timestamp}.json"

# Save to a file
save_transcriptions(wav2vec2bert_transcriptions, filename)

!ls /content/drive/MyDrive/STT

"""# Evaluating the performance of models using:"""

def load_transcriptions(filename):
    # Check if the file exists
    if not os.path.exists(filename):
        # If the file does not exist, return an empty dictionary
        return {}
    with open(filename, 'r') as f:
        return json.load(f)

# Load Wav2Vec2 transcriptions
wav2vec2bert_transcriptions = load_transcriptions("/content/drive/MyDrive/STT/wav2vec2bert_transcriptions20240828-133335.json")

print(wav2vec2bert_transcriptions)

transcriptions_dict = {

    "Wav2Vec2-BERT": wav2vec2bert_transcriptions
}

print(transcriptions_dict)

"""### Accuracy Metrics:

a. Word Error Rate (WER):
WER is commonly used for speech recognition tasks. It measures the edit distance between the predicted and reference transcriptions.

b. BLEU Score:
BLEU is typically used for translation tasks, but can also be applied to speech recognition when comparing against reference transcriptions.

c. Character Error Rate (CER):
Similar to WER, but operates at the character level instead of word level.

d. Phoneme Error Rate (PER):
Useful for evaluating phoneme-level accuracy, especially for low-resource languages.
"""

from datasets import load_metric
import pandas as pd
from google.colab import files

# Load the TSV file with reference transcriptions
uploaded = files.upload()
file_paths = list(uploaded.keys())
data = pd.read_csv(file_paths[0], delimiter='\t')
reference_transcriptions = dict(zip(data['path'], data['sentence']))

print(reference_transcriptions)

print(set(transcriptions_dict['Wav2Vec2-BERT'].values))
print(set(reference_transcriptions.values))

!pip install jiwer

# Initialize metrics
wer_metric = load_metric("wer")
bleu_metric = load_metric("bleu")
cer_metric = load_metric("cer")

def evaluate_transcriptions(transcriptions, reference_transcriptions):
    wer_scores, bleu_scores, cer_scores = {}, {}, {}
    for sentence_id, predicted_transcription in transcriptions.items():
        reference_transcription = reference_transcriptions.get(sentence_id)
        if reference_transcription:
          print(f"Evaluating: {sentence_id}")
          print(f"Predicted: {predicted_transcription}")
          print(f"Reference: {reference_transcription}")
          wer = wer_metric.compute(predictions=[predicted_transcription], references=[reference_transcription])
          bleu = bleu_metric.compute(predictions=[predicted_transcription.split()], references=[[reference_transcription.split()]])
          cer = cer_metric.compute(predictions=[predicted_transcription], references=[reference_transcription])

          wer_scores[sentence_id] = wer
          bleu_scores[sentence_id] = bleu['bleu']
          cer_scores[sentence_id] = cer
    return wer_scores, bleu_scores, cer_scores

# Evaluate all models
results = {}

for model_name, transcriptions in transcriptions_dict.items():
  print(f"Model: {model_name}, Transcriptions: {transcriptions}")
  wer_scores, bleu_scores, cer_scores = evaluate_transcriptions(transcriptions, reference_transcriptions)
  results[model_name] = {
      "WER": wer_scores,
      "BLEU": bleu_scores,
      "CER": cer_scores
  }

print(f"Reference Transcriptions Keys: {set(reference_transcriptions.keys())}")



transcription_ids = set(transcriptions_dict['Wav2Vec2-BERT'].keys())
reference_ids = set(reference_transcriptions.keys())

print(f"Transcription IDs: {transcription_ids}")
print(f"Reference IDs: {reference_ids}")
print(f"Common IDs: {transcription_ids.intersection(reference_ids)}")

import pandas as pd

def create_comparison_table(results):
  records = []
  for model_name, metrics in results.items():
    for sentence_id in metrics["WER"].keys():
      records.append({
          "Model": model_name,
          "Sentence ID": sentence_id,
          "WER": metrics["WER"].get(sentence_id, None),
          "BLEU": metrics["BLEU"].get(sentence_id, None),
          "CER": metrics["CER"].get(sentence_id, None),
        })
    df = pd.DataFrame(records)
  return df

# Create and display the comparison table
print(results)
comparison_df = create_comparison_table(results)
print(comparison_df)

# Optionally, save the comparison to a CSV file
comparison_df.to_csv('model_comparison.csv', index=False)

"""#################################################################################

Himakar Evaluation
"""

from google.colab import files

# Upload the reference file
uploaded = files.upload()

# Get the name of the uploaded file
reference_file_path = next(iter(uploaded))

# Load the reference transcriptions from the .txt file
reference_dict = {}

with open(reference_file_path, 'r', encoding='utf-8') as file:
    for line in file:
        parts = line.strip().split('\t')
        if len(parts) == 2:
            file_name, transcription = parts
            reference_dict[file_name] = transcription

print("Loaded reference transcriptions:", reference_transcriptions)

!pip install googletrans==4.0.0-rc1 # Install the googletrans library

from googletrans import Translator  # Import the Translator class from the googletrans library

translator = Translator()

# Function to translate text
def translate_text(text, src_language='auto', dest_language='en'):
  translation = translator.translate(text, src=src_language, dest=dest_language)
  return translation.text

# Translate reference transcriptions to English
translated_reference_dict = {
    sentence_id: translate_text(sentence)
    for sentence_id, sentence in reference_transcriptions.items()
}

# Translate model transcriptions to English (for Wav2Vec2-BERT model specifically)
translated_transcriptions = {
    file_name: translate_text(transcription)
    for file_name, transcription in transcriptions_dict['Wav2Vec2-BERT'].items()
}

# Output the translations for verification
print("Translated reference transcriptions:", translated_reference_dict)
print("Translated model transcriptions (Wav2Vec2-BERT):", translated_transcriptions)

from googletrans import Translator # Import the Translator class from the googletrans library

translator = Translator()

def translate_text(text, src_language='auto', dest_language='en'):
    translation = translator.translate(text, src=src_language, dest=dest_language)
    return translation.text

# Translate reference and model transcriptions to English
translated_reference_dict = {file_name: translate_text(text) for file_name, text in reference_transcriptions.items()}
translated_transcriptions = {file_path: translate_text(transcription) for file_path, transcription in transcriptions_dict.items()} #Change here for bert


print("Translated reference transcriptions:", translated_reference_dict)
print("Translated model transcriptions:", translated_transcriptions)

import jiwer

def evaluate_wer(transcriptions, reference_dict):
    wer_scores = {}

    for file_path, transcription in transcriptions.items():
        file_name = file_path.split('/')[-1]
        if file_name in reference_dict:
            reference_text = reference_dict[file_name]
            wer = jiwer.wer(reference_text, transcription)
            wer_scores[file_path] = wer

    return wer_scores

def evaluate_cer(transcriptions, reference_dict):
    cer_scores = {}

    for file_path, transcription in transcriptions.items():
        file_name = file_path.split('/')[-1]
        if file_name in reference_dict:
            reference_text = reference_dict[file_name]
            cer = jiwer.cer(reference_text, transcription)
            cer_scores[file_path] = cer

    return cer_scores

import nltk
from nltk.translate.bleu_score import sentence_bleu

def evaluate_bleu(transcriptions, reference_dict):
    bleu_scores = {}

    for file_path, transcription in transcriptions.items():
        file_name = file_path.split('/')[-1]
        if file_name in reference_dict:
            reference_text = reference_dict[file_name]
            reference_tokens = [reference_text.split()]
            hypothesis_tokens = transcription.split()

            bleu_score = sentence_bleu(reference_tokens, hypothesis_tokens)
            bleu_scores[file_path] = bleu_score

    return bleu_scores

# Evaluate metrics
wer_scores = evaluate_wer(transcriptions_dict, translated_reference_dict)
cer_scores = evaluate_cer(translated_transcriptions, translated_reference_dict)
bleu_scores = evaluate_bleu(translated_transcriptions, translated_reference_dict)

print("WER Scores:", wer_scores)
print("CER Scores:", cer_scores)
print("BLEU Scores:", bleu_scores)

import time
import psutil

def measure_performance(processed_files):
    transcription_times = {}
    cpu_usages = {}
    mem_usages = {}

    # Get initial CPU and memory usage
    process = psutil.Process()
    initial_cpu = psutil.cpu_percent(interval=None)
    initial_mem = process.memory_info().rss

    for file_path, audio, sr in processed_files:
        if sr != 16000:
            continue  # Wav2Vec2 requires 16kHz

        start_time = time.time()

        inputs = wav2vec2_processor(audio, sampling_rate=sr, return_tensors="pt")
        with torch.no_grad():
            logits = wav2vec2_model(inputs.input_values).logits
            predicted_ids = torch.argmax(logits, dim=-1)
        transcription = wav2vec2_processor.batch_decode(predicted_ids)[0]

        end_time = time.time()
        elapsed_time = end_time - start_time

        # Record CPU and memory usage
        current_cpu = psutil.cpu_percent(interval=None)
        current_mem = process.memory_info().rss

        transcription_times[file_path] = elapsed_time
        cpu_usages[file_path] = current_cpu - initial_cpu
        mem_usages[file_path] = (current_mem - initial_mem) / (1024 * 1024)  # Convert bytes to MB

        # Update initial values for next iteration
        initial_cpu = current_cpu
        initial_mem = current_mem

        print(f"Transcribed {file_path}")
        print(f"Transcription: {transcription}")
        print(f"Time taken: {elapsed_time:.2f} seconds")
        print(f"CPU usage: {cpu_usages[file_path]:.2f} %")
        print(f"Memory usage: {mem_usages[file_path]:.2f} MB")
        print("---")

    return transcription_times, cpu_usages, mem_usages

# Example usage
transcription_times, cpu_usages, mem_usages = measure_performance(processed_files)

print("Transcription Times:", transcription_times)
print("CPU Usages:", cpu_usages)
print("Memory Usages:", mem_usages)

"""#############################################chatgpt#################################"""

from google.colab import files
from googletrans import Translator  # Import the Translator class from the googletrans library
import jiwer
import nltk
from nltk.translate.bleu_score import sentence_bleu

# Upload the reference file
uploaded = files.upload()

# Get the name of the uploaded file
reference_file_path = next(iter(uploaded))

# Load the reference transcriptions from the .txt file
reference_transcriptions = {}

with open(reference_file_path, 'r', encoding='utf-8') as file:
    for line in file:
        parts = line.strip().split('\t')
        if len(parts) == 2:
            sentence_id, transcription = parts
            reference_transcriptions[sentence_id] = transcription

print("Loaded reference transcriptions:", reference_transcriptions)

# Initialize the translator
translator = Translator()

# Function to translate text
def translate_text(text, src_language='auto', dest_language='en'):
    translation = translator.translate(text, src=src_language, dest=dest_language)
    return translation.text

# Translate reference transcriptions to English
translated_reference_dict = {
    sentence_id: translate_text(sentence)
    for sentence_id, sentence in reference_transcriptions.items()
}

# Translate model transcriptions to English (for Wav2Vec2-BERT model specifically)
translated_transcriptions = {
    sentence_id: translate_text(transcription)
    for sentence_id, transcription in transcriptions_dict['Wav2Vec2-BERT'].items()
}

# Output the translations for verification
print("Translated reference transcriptions:", translated_reference_dict)
print("Translated model transcriptions (Wav2Vec2-BERT):", translated_transcriptions)

# Function to evaluate WER
def evaluate_wer(transcriptions, reference_dict):
    wer_scores = {}
    for sentence_id, transcription in transcriptions.items():
        if sentence_id in reference_dict:
            reference_text = reference_dict[sentence_id]
            wer = jiwer.wer(reference_text, transcription)
            wer_scores[sentence_id] = wer
    return wer_scores

# Function to evaluate CER
def evaluate_cer(transcriptions, reference_dict):
    cer_scores = {}
    for sentence_id, transcription in transcriptions.items():
        if sentence_id in reference_dict:
            reference_text = reference_dict[sentence_id]
            cer = jiwer.cer(reference_text, transcription)
            cer_scores[sentence_id] = cer
    return cer_scores

# Function to evaluate BLEU
def evaluate_bleu(transcriptions, reference_dict):
    bleu_scores = {}
    for sentence_id, transcription in transcriptions.items():
        if sentence_id in reference_dict:
            reference_text = reference_dict[sentence_id]
            reference_tokens = [reference_text.split()]
            hypothesis_tokens = transcription.split()
            bleu_score = sentence_bleu(reference_tokens, hypothesis_tokens)
            bleu_scores[sentence_id] = bleu_score
    return bleu_scores

# Evaluate metrics
wer_scores = evaluate_wer(translated_transcriptions, translated_reference_dict)
cer_scores = evaluate_cer(translated_transcriptions, translated_reference_dict)
bleu_scores = evaluate_bleu(translated_transcriptions, translated_reference_dict)

print("WER Scores:", wer_scores)
print("CER Scores:", cer_scores)
print("BLEU Scores:", bleu_scores)

"""################################################################################################################################# Seamless model ########################################################################

"""

from transformers import AutoProcessor, SeamlessM4Tv2Model
import torch

# Load SeamlessM4T-v2 model and processor
SeamlessM4T_v2_processor = AutoProcessor.from_pretrained("facebook/seamless-m4t-v2-large")
SeamlessM4T_v2_model = SeamlessM4Tv2Model.from_pretrained("facebook/seamless-m4t-v2-large")

def transcribe_audio_files_seamless(processed_files):

    transcriptions = {}

    for item in processed_files:
        try:
            file_path, preprocessed_audio, sample_rate = item
            # Prepare inputs
            inputs = SeamlessM4T_v2_processor(preprocessed_audio, return_tensors="pt", sampling_rate=sample_rate)

            # Ensure the model is in evaluation mode
            SeamlessM4T_v2_model.eval()

            # Generate transcription
            with torch.no_grad():
                output = SeamlessM4T_v2_model.generate(**inputs)

            # Decode the transcription
            transcription = SeamlessM4T_v2_processor.decode(output[0], skip_special_tokens=True)
            transcriptions[file_path] = transcription

            print(f"Transcribed {file_path}")
            print(f"Transcription: {transcription}")
            print("---")

        except ValueError as e:
            print(f"Skipping item due to error: {e}. Item content: {item}")

    return transcriptions

seamless_m4t_transcriptions = transcribe_audio_files_seamless(processed_files)

# Generate a unique filename based on the current time
import time
import json # Add this import statement to allow the use of json.dump()

timestamp = time.strftime("%Y%m%d-%H%M%S")
filename = f"/content/seamless_m4t_transcriptions_{timestamp}.json" # Changed the file path to a location that exists and the user has write access to.

# Save to a file
# Add the definition for the function save_transcriptions
def save_transcriptions(transcriptions, filename):
    with open(filename, 'w') as f:
        json.dump(transcriptions, f, indent=4)

save_transcriptions(seamless_m4t_transcriptions, filename)

!ls /content

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import torch
import numpy as np

# Initialize the processor and model for SeamlessM4T
model_name = "facebook/seamless_m4t-large"  # Replace with the correct model path if different
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

def transcribe_with_seamlessm4t(processed_files):
    transcriptions = {}
    for file_path, audio, sr in processed_files:
        if sr != 16000:
            print(f"Skipping {file_path}: sample rate {sr} is not 16kHz")
            continue  # SeamlessM4T may require a specific sample rate

        # Convert audio to list if needed
        if isinstance(audio, np.ndarray):
            audio = audio.astype(np.float32).tolist()

        # Tokenize and process audio input
        inputs = tokenizer(audio, return_tensors="pt", padding=True)

        with torch.no_grad():
            outputs = model.generate(**inputs)
            transcription = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]

        transcriptions[file_path] = transcription
        print(f"Transcribed {file_path}")
        print(f"Transcription: {transcription}")
        print("---")

    return transcriptions

# Example usage
# Assuming `processed_files` is a list of tuples (file_path, audio, sr)
processed_files = [
    ('common_voice_hi_26069894.mp3', np.random.randn(16000*5).astype(np.float32), 16000),
    # Add more tuples here as needed
]

seamlessm4t_transcriptions = transcribe_with_seamlessm4t(processed_files)
print(seamlessm4t_transcriptions)

!pip install transformers
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import torch
import numpy as np

# Initialize the processor and model for SeamlessM4T
# Use the correct model identifier.
model_name = "facebook/hf-seamless-m4t-large"  # Changed to the correct model_name
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

def transcribe_with_seamlessm4t(processed_files):
    transcriptions = {}
    for file_path, audio, sr in processed_files:
        if sr != 16000:
            print(f"Skipping {file_path}: sample rate {sr} is not 16kHz")
            continue  # SeamlessM4T may require a specific sample rate

        # Convert audio to list if needed
        if isinstance(audio, np.ndarray):
            audio = audio.astype(np.float32).tolist()

        # Tokenize and process audio input
        inputs = tokenizer(audio, return_tensors="pt", padding=True)

        with torch.no_grad():
            outputs = model.generate(**inputs)
            transcription = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]

        transcriptions[file_path] = transcription
        print(f"Transcribed {file_path}")
        print(f"Transcription: {transcription}")
        print("---")

    return transcriptions

# Example usage
# Assuming `processed_files` is a list of tuples (file_path, audio, sr)
processed_files = [
    ('common_voice_hi_26069894.mp3', np.random.randn(16000*5).astype(np.float32), 16000),
    # Add more tuples here as needed
]

seamlessm4t_transcriptions = transcribe_with_seamlessm4t(processed_files)
print(seamlessm4t_transcriptions)