# -*- coding: utf-8 -*-
"""Seamless.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cP2VTyaXiLY_1e8QvGeCEjMJF4bvvnJ-

# **Step 1: Data Preprocessing**

Converting the Audio files into WAV format andand resample to 16kHz. Converting to mono if they are in stereo format.Using librosa to load and resample audio.
"""

!pip install datasets transformers torch

"""Necessary imports"""

import librosa
import soundfile as sf
import os
import numpy as np
from IPython.display import Audio, display

"""## Load files

### Load audio files
"""

def load_audio(file_paths):
    """
    Load audio files.

    :param file_paths: list of str, path(s) to audio file(s)
    :return: list of tuples (file_path, audio_array, sample_rate)
    """
    loaded_files = []

    for file_path in file_paths:
        # Load audio file
        audio, sr = librosa.load(file_path, sr=None, mono=False)
        loaded_files.append((file_path, audio, sr))

        print(f"Loaded {file_path}")
        print(f"Shape: {audio.shape}, Sample rate: {sr}")
        print("---")

    return loaded_files

"""**usage**"""

# Load audio files
# loaded_files = load_audio("downloads/telugu30min.wav", "downloads/english30min.wav")
# Change to your file system paths
# Can use single or multiple files at a time

"""You can also directly upload the files usind this"""

from google.colab import files

uploaded = files.upload()  # This will prompt you to upload files

file_paths = list(uploaded.keys())

# Load audio files
loaded_files = load_audio(file_paths)

"""### Batch loading

Do not run this if you already loaded files
"""

# this will open the validated.tsv file, collect the file paths and sentences
# make folder path wherever the common voice language files are located
import pandas as pd

folder_path = 'data/tig/'
valid_path = folder_path + 'validated.tsv'
validated_data = pd.read_csv(valid_path, sep="\t")
validated_data = validated_data.loc[:,['path','sentence']]

def load_audio_auto(valid_data, folder_path):
    # takes the file paths and folder path and loads the validated audio clips
    # returns a list of tuples (file_path, audio_array, sample_rate)
    loaded_files = []

    for file_path in valid_data.loc[:,'path']:
        # Load audio file
        audio, sr = librosa.load(folder_path+'clips/'+file_path, sr=None, mono=False)
        loaded_files.append((file_path, audio, sr))

    return loaded_files

loaded_files = load_audio_auto(validated_data, folder_path)

"""## Data **Preprocessing**"""

def preprocess_audio(loaded_files, target_sr=16000, mono=True):
    """
    Preprocess loaded audio files and display them for listening.

    :param loaded_files: list of tuples (file_path, audio_array, sample_rate)
    :param target_sr: int, target sample rate (default: 16000)
    :param mono: bool, whether to convert to mono (default: True)
    :return: list of tuples (original_path, processed_audio_array, sr)
    """
    processed_files = []

    for file_path, audio, sr in loaded_files:
        original_filename = os.path.splitext(os.path.basename(file_path))[0]

        # Display original audio
        print(f"Original audio: {original_filename}")
        display(Audio(audio, rate=sr))

        # Resample if necessary
        if sr != target_sr:
            audio = librosa.resample(y=audio, orig_sr=sr, target_sr=target_sr)

        # Convert to mono if required
        if mono and audio.ndim > 1:
            audio = librosa.to_mono(audio)

        processed_files.append((file_path, audio, target_sr))

        # Display processed audio
        print(f"Processed audio: {original_filename}")
        display(Audio(audio, rate=target_sr))

        print(f"Processed {file_path}")
        print(f"Shape: {audio.shape}, Sample rate: {target_sr}")
        print("---")

    return processed_files

# Process loaded audio files
processed_files = preprocess_audio(loaded_files, target_sr=16000, mono=True)

"""# **Data Augmentation Techniques**

Do not worry about it now
"""

import torch
import torchaudio
from transformers import Wav2Vec2Processor
import soundfile as sf

"""### Add background noise"""

def add_noise(audio, noise_factor=0.005):
    noise = np.random.randn(len(audio))
    return audio + noise_factor * noise

def apply_noise_to_processed_files(processed_files, noise_factor=0.005):
    augmented_files = []

    for file_path, audio, sr in processed_files:
        augmented_audio = add_noise(audio, noise_factor)
        augmented_files.append((file_path, augmented_audio, sr))

        original_filename = os.path.splitext(os.path.basename(file_path))[0]

        # Display original audio
        print(f"Original audio: {original_filename}")
        display(Audio(audio, rate=sr))

        # Display augmented audio
        print(f"Augmented audio: {original_filename}")
        display(Audio(augmented_audio, rate = target_sr))

        print(f"Applied noise to {file_path}")
        print(f"Original shape: {audio.shape}, Augmented shape: {augmented_audio.shape}")
        print("---")

    return augmented_files

# Usage
target_sr=16000
augmented_audio = apply_noise_to_processed_files(processed_files)

"""Speed perturbation"""

def change_speed(audio, speed_factor):
    return librosa.effects.time_stretch(audio, rate=speed_factor)

def apply_speed_to_processed_files(processed_files, speed_factor=1.0):
    augmented_files = []

    for file_path, audio, sr in processed_files:
        original_filename = os.path.splitext(os.path.basename(file_path))[0]

        # Display original audio
        print(f"Original audio: {original_filename}")
        display(Audio(audio, rate=sr))

        # Apply speed change
        augmented_audio = change_speed(audio, speed_factor)
        augmented_files.append((file_path, augmented_audio, sr))

        # Display augmented audio
        print(f"Augmented audio: {original_filename}")
        display(Audio(augmented_audio, rate=sr))

        print(f"Applied augmentations to {file_path}")
        print(f"Original shape: {audio.shape}, Augmented shape: {augmented_audio.shape}")
        print("---")

    return augmented_files

# Usage
speed_changed_audio = apply_speed_to_processed_files(processed_files, speed_factor=1.2)  # 20% faster

"""Pitch shifting"""

def pitch_shift(audio, sr, n_steps):
    return librosa.effects.pitch_shift(audio, sr=sr, n_steps=n_steps)

def shift_pitch_to_processed_files(processed_files, sr, n_steps):
    augmented_files = []

    for file_path, audio, sr in processed_files:
        original_filename = os.path.splitext(os.path.basename(file_path))[0]

        # Display original audio
        print(f"Original audio: {original_filename}")
        display(Audio(audio, rate=sr))

        # Apply pitch shift
        augmented_audio = pitch_shift(audio, sr, n_steps)
        augmented_files.append((file_path, augmented_audio, sr))

        # Display augmented audio
        print(f"Augmented audio: {original_filename}")
        display(Audio(augmented_audio, rate=sr))

        print(f"Applied augmentations to {file_path}")
        print(f"Original shape: {audio.shape}, Augmented shape: {augmented_audio.shape}")
        print("---")

# Usage
pitch_shifted_audio = shift_pitch_to_processed_files(processed_files, target_sr, n_steps=2)  # Shift pitch up by 2 semitones

"""SpecAugment (time and frequency masking)"""

import random
def spec_augment(spec, num_mask=2, freq_masking_max_percentage=0.15, time_masking_max_percentage=0.3):
    spec = spec.copy()
    for i in range(num_mask):
        all_frames_num, all_freqs_num = spec.shape
        freq_percentage = random.uniform(0.0, freq_masking_max_percentage)
        num_freqs_to_mask = int(freq_percentage * all_freqs_num)
        f0 = np.random.uniform(low=0.0, high=all_freqs_num - num_freqs_to_mask)
        f0 = int(f0)
        spec[:, f0:f0 + num_freqs_to_mask] = 0
        time_percentage = random.uniform(0.0, time_masking_max_percentage)
        num_frames_to_mask = int(time_percentage * all_frames_num)
        t0 = np.random.uniform(low=0.0, high=all_frames_num - num_frames_to_mask)
        t0 = int(t0)
        spec[t0:t0 + num_frames_to_mask, :] = 0
    return spec

def apply_spec_augment_to_processed_files(processed_files, num_mask=2, freq_masking_max_percentage=0.15, time_masking_max_percentage=0.3):
    augmented_files = []

    for file_path, audio, sr in processed_files:
        original_filename = os.path.splitext(os.path.basename(file_path))[0]

        # Display original audio
        print(f"Original audio: {original_filename}")
        display(Audio(audio, rate=sr))

        # Convert to mel spectrogram
        mel_spec = librosa.feature.melspectrogram(y=audio, sr=sr)

        # Apply spectral augmentation
        augmented_spec = spec_augment(mel_spec, num_mask, freq_masking_max_percentage, time_masking_max_percentage)

        # Convert back to audio
        augmented_audio = librosa.feature.inverse.mel_to_audio(augmented_spec, sr=sr)

        augmented_files.append((file_path, augmented_audio, sr))

        # Display augmented audio
        print(f"Augmented audio: {original_filename}")
        display(Audio(augmented_audio, rate=sr))

        print(f"Applied spectral augmentation to {file_path}")
        print(f"Original shape: {audio.shape}, Augmented shape: {augmented_audio.shape}")
        print("---")

    return augmented_files

# Apply spectral augmentation to all files
augmented_files = apply_spec_augment_to_processed_files(
        processed_files,
        num_mask=2,
        freq_masking_max_percentage=0.15,
        time_masking_max_percentage=0.3)

"""Combining augmentations -- do not run this part. Need to be updated."""

def augment_audio(audio, sr, impulse_response=None):
    augmentations = [
        add_noise,
        lambda x: change_speed(x, np.random.uniform(0.9, 1.1)),
        lambda x: pitch_shift(x, sr, np.random.randint(-2, 3)),
    ]

    if impulse_response is not None:
        augmentations.append(lambda x: apply_impulse_response(x, impulse_response))

    # Randomly choose 2-3 augmentations
    num_augs = np.random.randint(2, 4)
    chosen_augmentations = np.random.choice(augmentations, num_augs, replace=False)

    for aug_func in chosen_augmentations:
        audio = aug_func(audio)

    # Always apply spec augment as the last step
    mel_spec = librosa.feature.melspectrogram(y=audio, sr=sr)
    augmented_spec = spec_augment(mel_spec)
    audio = librosa.feature.inverse.mel_to_audio(augmented_spec, sr=sr)

    return audio

"""Room impulse response simulation"""





"""# Testing Models"""

from huggingface_hub import notebook_login

notebook_login()

"""## Model Transcription saving"""

import json
import time

def save_transcriptions(transcriptions, filename):

    directory = os.path.dirname(filename)
    if not os.path.exists(directory):
        os.makedirs(directory)

    with open(filename, 'w') as f:
        json.dump(transcriptions, f, indent=4)

"""### 1. **Whisper**:

Data preprocessing:

a. Audio files should be in WAV format, 16-bit PCM.

b. Use librosa or pydub to load and resample audio to 16kHz if necessary.

Data augmentation:

a. Add background noise

b. Apply speed perturbation

c. Simulate room acoustics

Install the required modules before running the **models**
"""

from transformers import WhisperForConditionalGeneration, WhisperProcessor
import torch

# model_name = "openai/whisper-medium"
# model_name = "openai/whisper-large-v3"

# Load Whisper model and processor
Whisper_processor = WhisperProcessor.from_pretrained("openai/whisper-small")
Whisper_model = WhisperForConditionalGeneration.from_pretrained("openai/whisper-small")

"""https://huggingface.co/carlosdanielhernandezmena/whisper-largev2-faroese-8k-steps-100h/blob/main/README.md

refered to build the whisper model.

**Note: Pre trained model does not know my language. Converting to Hindi directly. Need to fine tune and train for better usage.**

Large model able to translate good enough but need a lot of time. Comparitively lot better than small model. But need high computational resources.
"""

def transcribe_audio_files_whisper(processed_files):

    transcriptions = {}

    for file_path, audio, sr in processed_files:
        # Preprocess audio
        inputs = Whisper_processor(audio, sampling_rate=sr, return_tensors="pt")
        audio_tensor = inputs.input_features  # Use 'input_features' instead of 'input_values'

        # Ensure the model is in evaluation mode
        Whisper_model.eval()

        # Generate transcription
        with torch.no_grad():
            output = Whisper_model.generate(audio_tensor)

        # Decode the transcription
        transcription = Whisper_processor.decode(output[0], skip_special_tokens=True)
        transcriptions[file_path] = transcription

        print(f"Transcribed {file_path}")
        print(f"Transcription: {transcription}")
        print("---")

    return transcriptions

whisper_transcriptions = transcribe_audio_files_whisper(processed_files)

# Generate a unique filename based on the current time
timestamp = time.strftime("%Y%m%d-%H%M%S")
filename = f"/content/drive/MyDrive/STT/whisper_transcriptions_{timestamp}.json"

# Save to a file
save_transcriptions(whisper_transcriptions, filename)

!ls /content/drive/MyDrive/STT

"""### **2. Wav2vec2**"""

from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC
import torch

 # load model and tokenizer
wav2vec2_processor = Wav2Vec2Processor.from_pretrained("facebook/wav2vec2-large-960h-lv60-self")
wav2vec2_model = Wav2Vec2ForCTC.from_pretrained("facebook/wav2vec2-large-960h-lv60-self")

def transcribe_with_wav2vec2(processed_files):
    transcriptions = {}
    for file_path, audio, sr in processed_files:
        if sr != 16000:
            continue  # Wav2Vec2 requires 16kHz
        inputs = wav2vec2_processor(audio, sampling_rate=sr, return_tensors="pt")
        with torch.no_grad():
            # Use 'input_values' instead of 'input_features'
            logits = wav2vec2_model(inputs.input_values).logits
            predicted_ids = torch.argmax(logits, dim=-1)
        transcription = wav2vec2_processor.batch_decode(predicted_ids)[0]
        transcriptions[file_path] = transcription
        print(f"Transcribed {file_path}")
        print(f"Transcription: {transcription}")
        print("---")
    return transcriptions

wav2vec2_transcriptions = transcribe_with_wav2vec2(processed_files)

# Generate a unique filename based on the current time
timestamp = time.strftime("%Y%m%d-%H%M%S")
filename = f"/content/drive/MyDrive/STT/wav2vec2_transcriptions_{timestamp}.json"

# Save to a file
save_transcriptions(wav2vec2_transcriptions, filename)

!ls /content/drive/MyDrive/STT

"""### 3. Wav2vec2-BERT"""

from transformers import AutoProcessor, Wav2Vec2BertForCTC
import torch
import numpy as np

# Initialize the processor and model
wav2vec2bert_processor = AutoProcessor.from_pretrained("hf-audio/wav2vec2-bert-CV16-en")
wav2vec2bert_model = Wav2Vec2BertForCTC.from_pretrained("hf-audio/wav2vec2-bert-CV16-en")

def transcribe_with_wav2vec2bert(processed_files):
    transcriptions = {}
    for file_path, audio, sr in processed_files:
        if sr != 16000:
            continue  # Wav2Vec2 requires 16kHz
        inputs = wav2vec2bert_processor(audio, sampling_rate=sr, return_tensors="pt")
        with torch.no_grad():
            logits = wav2vec2bert_model(inputs.input_features).logits
            predicted_ids = torch.argmax(logits, dim=-1)
        transcription = wav2vec2bert_processor.batch_decode(predicted_ids)[0]
        transcriptions[file_path] = transcription
        print(f"Transcribed {file_path}")
        print(f"Transcription: {transcription}")
        print("---")
    return transcriptions

wav2vec2bert_transcriptions = transcribe_with_wav2vec2bert(processed_files)

# Generate a unique filename based on the current time
timestamp = time.strftime("%Y%m%d-%H%M%S")
filename = f"/content/drive/MyDrive/STT/wav2vec2bert_transcriptions_{timestamp}.json"

# Save to a file
save_transcriptions(wav2vec2bert_transcriptions, filename)

!ls /content/drive/MyDrive/STT

"""### SeamlessM4T v2"""

from transformers import AutoProcessor, SeamlessM4Tv2Model
import torch

# Load SeamlessM4T-v2 model and processor
SeamlessM4T_v2_processor = AutoProcessor.from_pretrained("facebook/seamless-m4t-v2-large")
SeamlessM4T_v2_model = SeamlessM4Tv2Model.from_pretrained("facebook/seamless-m4t-v2-large")

def transcribe_audio_files_seamless(processed_files):

    transcriptions = {}

    for item in processed_files:
        try:
            file_path, preprocessed_audio, sample_rate = item
            # Prepare inputs
            inputs = SeamlessM4T_v2_processor(preprocessed_audio, return_tensors="pt", sampling_rate=sample_rate)

            # Ensure the model is in evaluation mode
            SeamlessM4T_v2_model.eval()

            # Generate transcription
            with torch.no_grad():
                output = SeamlessM4T_v2_model.generate(**inputs)

            # Decode the transcription
            transcription = SeamlessM4T_v2_processor.decode(output[0], skip_special_tokens=True)
            transcriptions[file_path] = transcription

            print(f"Transcribed {file_path}")
            print(f"Transcription: {transcription}")
            print("---")

        except ValueError as e:
            print(f"Skipping item due to error: {e}. Item content: {item}")

    return transcriptions

seamless_m4t_transcriptions = transcribe_audio_files_seamless(processed_files)

# Generate a unique filename based on the current time
timestamp = time.strftime("%Y%m%d-%H%M%S")
filename = f"/content/drive/MyDrive/STT/seamless_m4t_transcriptions_{timestamp}.json"

# Save to a file
save_transcriptions(seamless_m4t_transcriptions, filename)

!ls /content/drive/MyDrive/STT

"""# Evaluating the performance of models using:"""

def load_transcriptions(filename):
    with open(filename, 'r') as f:
        return json.load(f)

# Load Whisper transcriptions
whisper_transcriptions = load_transcriptions("/content/drive/MyDrive/STT/whisper_transcriptions_20240828-041646.json")

# Load Wav2Vec2 transcriptions
wav2vec2_transcriptions = load_transcriptions("/content/drive/MyDrive/STT/wav2vec2_transcriptions_20240828-041655.json")

# Load Wav2Vec2 transcriptions
wav2vec2bert_transcriptions = load_transcriptions("/content/drive/MyDrive/STT/wav2vec2bert_transcriptions_20240828-041741.json")

# Load Wav2Vec2 transcriptions
seamless_m4t_transcriptions = load_transcriptions("/content/drive/MyDrive/STT/seamless_m4t_transcriptions_20240828-041753.json")

transcriptions_dict = {
    "Whisper": whisper_transcriptions,
    "Wav2Vec2": wav2vec2_transcriptions,
    "Wav2Vec2-BERT": wav2vec2bert_transcriptions,
    "seamless_m4t": seamless_m4t_transcriptions

}

"""### Accuracy Metrics:

a. Word Error Rate (WER):
WER is commonly used for speech recognition tasks. It measures the edit distance between the predicted and reference transcriptions.

b. BLEU Score:
BLEU is typically used for translation tasks, but can also be applied to speech recognition when comparing against reference transcriptions.

c. Character Error Rate (CER):
Similar to WER, but operates at the character level instead of word level.

d. Phoneme Error Rate (PER):
Useful for evaluating phoneme-level accuracy, especially for low-resource languages.
"""

from datasets import load_metric
import pandas as pd
from google.colab import files
import io  # Import the io module

# Load the TSV file with reference transcriptions
uploaded = files.upload()
file_name = list(uploaded.keys())[0]  # Get the filename
file_content = uploaded[file_name]   # Get the file contents as bytes

# Read the TSV file using io.BytesIO, selecting only 'path' and 'sentence' columns, and limiting to the first 5 rows
data = pd.read_csv(io.BytesIO(file_content), delimiter='\t', usecols=['path', 'sentence'], nrows=5)  # Read only first 5 rows

# Now you can access the 'path' and 'sentence' columns
reference_transcriptions = dict(zip(data['path'], data['sentence']))

print(reference_transcriptions)

!pip install jiwer

# Initialize metrics
wer_metric = load_metric("wer")
bleu_metric = load_metric("bleu")
cer_metric = load_metric("cer")

def evaluate_transcriptions(transcriptions, reference_transcriptions):
    wer_scores, bleu_scores, cer_scores = {}, {}, {}
    for sentence_id, predicted_transcription in transcriptions.items():
        reference_transcription = reference_transcriptions.get(sentence_id)
        if reference_transcription:
            wer = wer_metric.compute(predictions=[predicted_transcription], references=[reference_transcription])
            bleu = bleu_metric.compute(predictions=[predicted_transcription.split()], references=[[reference_transcription.split()]])
            cer = cer_metric.compute(predictions=[predicted_transcription], references=[reference_transcription])

            wer_scores[sentence_id] = wer
            bleu_scores[sentence_id] = bleu['bleu']
            cer_scores[sentence_id] = cer
    return wer_scores, bleu_scores, cer_scores

# Evaluate all models
results = {}

for model_name, transcriptions in transcriptions_dict.items():
    wer_scores, bleu_scores, cer_scores = evaluate_transcriptions(transcriptions, reference_transcriptions)
    results[model_name] = {
        "WER": wer_scores,
        "BLEU": bleu_scores,
        "CER": cer_scores
    }

"""Display and compare metrices"""

import pandas as pd

def create_comparison_table(results):
    # Flatten the results into a list of records
    records = []
    for model_name, metrics in results.items():
        for sentence_id in metrics["WER"].keys():
            records.append({
                "Model": model_name,
                "Sentence ID": sentence_id,
                "WER": metrics["WER"].get(sentence_id, None),
                "BLEU": metrics["BLEU"].get(sentence_id, None),
                "CER": metrics["CER"].get(sentence_id, None),
            })

    # Create a DataFrame
    df = pd.DataFrame(records)
    return df

# Create and display the comparison table
comparison_df = create_comparison_table(results)
print(comparison_df)

# Optionally, save the comparison to a CSV file
comparison_df.to_csv('model_comparison.csv', index=False)

"""Visuvalization"""

import seaborn as sns
import matplotlib.pyplot as plt

def visualize_comparison(comparison_df, metric):
    sns.barplot(data=comparison_df, x="Model", y=metric, hue="Sentence ID")
    plt.title(f"Comparison of {metric} Across Models")
    plt.show()

# Visualize WER comparison
visualize_comparison(comparison_df, "WER")

# Visualize BLEU comparison
visualize_comparison(comparison_df, "BLEU")

# Visualize CER comparison
visualize_comparison(comparison_df, "CER")

"""### Inference Speed and Resource Usage

To measure inference speed and resource usage, we'll use the time module and psutil library.
"""

pip install psutil

import time
import psutil
import torch
from transformers import WhisperForConditionalGeneration, WhisperProcessor, AutoProcessor, Wav2Vec2BertForCTC

# Initialize models and processors
whisper_processor = WhisperProcessor.from_pretrained("openai/whisper-large-v3")
whisper_model = WhisperForConditionalGeneration.from_pretrained("openai/whisper-large-v3")

wav2vec2_processor = AutoProcessor.from_pretrained("hf-audio/wav2vec2-bert-CV16-en")
wav2vec2_model = Wav2Vec2BertForCTC.from_pretrained("hf-audio/wav2vec2-bert-CV16-en")

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
whisper_model.to(device)
wav2vec2_model.to(device)

def measure_inference(model, processor, audio, sr):
    # Measure start time and initial resource usage
    start_time = time.time()
    process = psutil.Process()
    start_memory = process.memory_info().rss  # in bytes
    start_cpu = process.cpu_percent(interval=None)  # CPU percent at start

    # Process the input audio
    inputs = processor(audio, sampling_rate=sr, return_tensors="pt", task="translate").to(device)

    # Inference
    with torch.no_grad():
        if isinstance(model, WhisperForConditionalGeneration):
            input_features = inputs["input_features"]
            logits = model.generate(input_features)
            transcription = processor.decode(logits[0], skip_special_tokens=True)
        elif isinstance(model, Wav2Vec2BertForCTC):
            input_features = inputs["input_features"]
            logits = model(input_features=input_features).logits
            predicted_ids = torch.argmax(logits, dim=-1)
            transcription = processor.batch_decode(predicted_ids)[0]

    # Measure end time and final resource usage
    end_time = time.time()
    end_memory = process.memory_info().rss  # in bytes
    end_cpu = process.cpu_percent(interval=None)  # CPU percent at end

    # Calculate metrics
    elapsed_time = end_time - start_time
    memory_used = end_memory - start_memory  # in bytes
    cpu_usage = end_cpu - start_cpu

    return transcription, elapsed_time, memory_used, cpu_usage

# Measure inference speed and resource usage for each model
for file_name, audio, sr in processed_files:
    # Measure Whisper model inference
    whisper_transcription, whisper_time, whisper_memory, whisper_cpu = measure_inference(whisper_model, whisper_processor, audio, sr)
    print(f"Whisper Model - {file_name}:")
    print(f"  Transcription: {whisper_transcription}")
    print(f"  Time: {whisper_time:.4f} seconds")
    print(f"  Memory: {whisper_memory / (1024**2):.2f} MB")
    print(f"  CPU Usage: {whisper_cpu:.2f}%")
    print("---")

    # Measure Wav2Vec2-BERT model inference
    wav2vec2_transcription, wav2vec2_time, wav2vec2_memory, wav2vec2_cpu = measure_inference(wav2vec2_model, wav2vec2_processor, audio, sr)
    print(f"Wav2Vec2-BERT Model - {file_name}:")
    print(f"  Transcription: {wav2vec2_transcription}")
    print(f"  Time: {wav2vec2_time:.4f} seconds")
    print(f"  Memory: {wav2vec2_memory / (1024**2):.2f} MB")
    print(f"  CPU Usage: {wav2vec2_cpu:.2f}%")
    print("---")

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Collect data for visualization
data = []
for file_name, audio, sr in processed_files:
    # Whisper model data
    _, whisper_time, whisper_memory, whisper_cpu = measure_inference(whisper_model, whisper_processor, audio, sr)
    data.append({"Model": "Whisper", "File": file_name, "Time (s)": whisper_time, "Memory (MB)": whisper_memory / (1024**2), "CPU (%)": whisper_cpu})

    # Wav2Vec2-BERT model data
    _, wav2vec2_time, wav2vec2_memory, wav2vec2_cpu = measure_inference(wav2vec2_model, wav2vec2_processor, audio, sr)
    data.append({"Model": "Wav2Vec2-BERT", "File": file_name, "Time (s)": wav2vec2_time, "Memory (MB)": wav2vec2_memory / (1024**2), "CPU (%)": wav2vec2_cpu})

# Convert to DataFrame
df = pd.DataFrame(data)

# Visualize Inference Time
sns.barplot(x="File", y="Time (s)", hue="Model", data=df)
plt.title("Inference Time Comparison")
plt.show()

# Visualize Memory Usage
sns.barplot(x="File", y="Memory (MB)", hue="Model", data=df)
plt.title("Memory Usage Comparison")
plt.show()

# Visualize CPU Usage
sns.barplot(x="File", y="CPU (%)", hue="Model", data=df)
plt.title("CPU Usage Comparison")
plt.show()