{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ce4e885-4736-4a07-89ba-967bcb4c5885",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hemas\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-10-19 13:25:28,024 [INFO] Recording for 10 seconds at 16000 Hz...\n",
      "2024-10-19 13:25:38,206 [INFO] Saved raw audio to raw_audio\\raw_audio_combined.wav\n",
      "2024-10-19 13:25:38,208 [INFO] Cleaning audio: raw_audio\\raw_audio_combined.wav\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Couldn't find appropriate backend to handle uri raw_audio\\raw_audio_combined.wav and format None.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 141\u001b[0m\n\u001b[0;32m    138\u001b[0m     logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinal Transcription: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtranscription\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 141\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 132\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;66;03m# Step 2: Clean the recorded audio\u001b[39;00m\n\u001b[0;32m    131\u001b[0m audio_processor \u001b[38;5;241m=\u001b[39m AudioProcessor()\n\u001b[1;32m--> 132\u001b[0m cleaned_file_path \u001b[38;5;241m=\u001b[39m \u001b[43maudio_processor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclean_audio\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_file_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;66;03m# Step 3: Transcribe the cleaned audio\u001b[39;00m\n\u001b[0;32m    135\u001b[0m transcriber \u001b[38;5;241m=\u001b[39m SpeechToTextTranscriber()\n",
      "Cell \u001b[1;32mIn[1], line 71\u001b[0m, in \u001b[0;36mAudioProcessor.clean_audio\u001b[1;34m(self, raw_file_path)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclean_audio\u001b[39m(\u001b[38;5;28mself\u001b[39m, raw_file_path):\n\u001b[0;32m     70\u001b[0m     logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCleaning audio: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mraw_file_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 71\u001b[0m     waveform, sample_rate \u001b[38;5;241m=\u001b[39m \u001b[43mtorchaudio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_file_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;66;03m# Convert to mono\u001b[39;00m\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m waveform\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torchaudio\\_backend\\utils.py:204\u001b[0m, in \u001b[0;36mget_load_func.<locals>.load\u001b[1;34m(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size, backend)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\n\u001b[0;32m    119\u001b[0m     uri: Union[BinaryIO, \u001b[38;5;28mstr\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike],\n\u001b[0;32m    120\u001b[0m     frame_offset: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    126\u001b[0m     backend: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    127\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;28mint\u001b[39m]:\n\u001b[0;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load audio data from source.\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \n\u001b[0;32m    130\u001b[0m \u001b[38;5;124;03m    By default (``normalize=True``, ``channels_first=True``), this function returns Tensor with\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;124;03m            `[channel, time]` else `[time, channel]`.\u001b[39;00m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 204\u001b[0m     backend \u001b[38;5;241m=\u001b[39m \u001b[43mdispatcher\u001b[49m\u001b[43m(\u001b[49m\u001b[43muri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mload(uri, frame_offset, num_frames, normalize, channels_first, \u001b[38;5;28mformat\u001b[39m, buffer_size)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torchaudio\\_backend\\utils.py:116\u001b[0m, in \u001b[0;36mget_load_func.<locals>.dispatcher\u001b[1;34m(uri, format, backend_name)\u001b[0m\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mcan_decode(uri, \u001b[38;5;28mformat\u001b[39m):\n\u001b[0;32m    115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m backend\n\u001b[1;32m--> 116\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find appropriate backend to handle uri \u001b[39m\u001b[38;5;132;01m{\u001b[39;00muri\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and format \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mformat\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Couldn't find appropriate backend to handle uri raw_audio\\raw_audio_combined.wav and format None."
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "import noisereduce as nr\n",
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "from scipy.io.wavfile import write\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"transcription.log\"),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Step 1: Audio Recorder\n",
    "class AudioRecorder:\n",
    "    def __init__(self):\n",
    "        self.sample_rate = 16000\n",
    "        self.channels = 1\n",
    "        self.audio_queue = []\n",
    "\n",
    "    def list_devices(self):\n",
    "        \"\"\"Lists all available audio devices.\"\"\"\n",
    "        devices = sd.query_devices()\n",
    "        for i, device in enumerate(devices):\n",
    "            print(f\"{i}: {device['name']}\")\n",
    "\n",
    "    def audio_callback(self, indata, frames, time, status):\n",
    "        self.audio_queue.append(indata.copy())\n",
    "\n",
    "    def record_audio(self, duration=10, device_name=\"default\"):\n",
    "        logging.info(f\"Recording for {duration} seconds at {self.sample_rate} Hz...\")\n",
    "        \n",
    "        # Find the correct device by name\n",
    "        device_id = None\n",
    "        for i, device in enumerate(sd.query_devices()):\n",
    "            if device_name in device['name']:\n",
    "                device_id = i\n",
    "                break\n",
    "\n",
    "        if device_id is None:\n",
    "            raise ValueError(f\"Device '{device_name}' not found\")\n",
    "        \n",
    "        with sd.InputStream(callback=self.audio_callback, channels=self.channels, samplerate=self.sample_rate, device=device_id):\n",
    "            sd.sleep(int(duration * 1000))\n",
    "        \n",
    "        # Combine all the chunks and save as a single WAV file\n",
    "        combined_audio = np.concatenate(self.audio_queue, axis=0)\n",
    "        raw_audio_folder = \"raw_audio\"\n",
    "        os.makedirs(raw_audio_folder, exist_ok=True)\n",
    "        raw_file_path = os.path.join(raw_audio_folder, \"raw_audio_combined.wav\")\n",
    "        write(raw_file_path, self.sample_rate, combined_audio.astype(np.float32))\n",
    "        logging.info(f\"Saved raw audio to {raw_file_path}\")\n",
    "        return raw_file_path\n",
    "\n",
    "# Step 2: Audio Processing\n",
    "class AudioProcessor:\n",
    "    def __init__(self, output_dir=\"cleaned_audio\"):\n",
    "        self.output_dir = output_dir\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "\n",
    "    def clean_audio(self, raw_file_path):\n",
    "        logging.info(f\"Cleaning audio: {raw_file_path}\")\n",
    "        waveform, sample_rate = torchaudio.load(raw_file_path)\n",
    "\n",
    "        # Convert to mono\n",
    "        if waveform.shape[0] > 1:\n",
    "            waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "\n",
    "        # Apply noise reduction\n",
    "        waveform_np = waveform.numpy()\n",
    "        reduced_noise_waveform = nr.reduce_noise(y=waveform_np, sr=sample_rate)\n",
    "\n",
    "        # Convert back to tensor\n",
    "        cleaned_waveform_tensor = torch.tensor(reduced_noise_waveform, dtype=torch.float32)\n",
    "\n",
    "        # Save cleaned audio\n",
    "        cleaned_file_path = os.path.join(self.output_dir, \"cleaned_audio.wav\")\n",
    "        torchaudio.save(cleaned_file_path, cleaned_waveform_tensor, sample_rate)\n",
    "        logging.info(f\"Saved cleaned audio to {cleaned_file_path}\")\n",
    "        return cleaned_file_path\n",
    "\n",
    "# Step 3: Speech-to-Text Transcription\n",
    "class SpeechToTextTranscriber:\n",
    "    def __init__(self):\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        torch_dtype = torch.float32\n",
    "        model_id = \"distil-whisper/distil-large-v3\"\n",
    "\n",
    "        # Load the Whisper model and processor\n",
    "        self.model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "            model_id, torch_dtype=torch_dtype, use_safetensors=True\n",
    "        ).to(device)\n",
    "        self.processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "        # Create a speech recognition pipeline\n",
    "        self.pipe = pipeline(\n",
    "            \"automatic-speech-recognition\",\n",
    "            model=self.model,\n",
    "            tokenizer=self.processor.tokenizer,\n",
    "            feature_extractor=self.processor.feature_extractor,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "    def transcribe(self, cleaned_audio_file):\n",
    "        logging.info(f\"Transcribing {cleaned_audio_file}...\")\n",
    "        result = self.pipe(cleaned_audio_file, return_timestamps=\"word\")\n",
    "        text = result['text']\n",
    "        logging.info(f\"Transcription: {text}\")\n",
    "        return text\n",
    "\n",
    "# Main execution function\n",
    "def main():\n",
    "    # Step 1: Record audio\n",
    "    recorder = AudioRecorder()\n",
    "\n",
    "    # Call the list of devices to select the right device\n",
    "    #recorder.list_devices()\n",
    "\n",
    "    # Specify the correct device name found in the list\n",
    "    raw_file_path = recorder.record_audio(duration=10, device_name=\"Microphone Array (IntelÂ® Smart Sound Technology for Digital Microphones)\")\n",
    "\n",
    "    # Step 2: Clean the recorded audio\n",
    "    audio_processor = AudioProcessor()\n",
    "    cleaned_file_path = audio_processor.clean_audio(raw_file_path)\n",
    "\n",
    "    # Step 3: Transcribe the cleaned audio\n",
    "    transcriber = SpeechToTextTranscriber()\n",
    "    transcription = transcriber.transcribe(cleaned_file_path)\n",
    "\n",
    "    logging.info(f\"Final Transcription: {transcription}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f272be76-630b-4f31-b91c-d607e5561bfb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
