{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        ">Evaluating the performance of models using:\n",
        "\n"
      ],
      "metadata": {
        "id": "9E8MDq0Li1FS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install jiwer"
      ],
      "metadata": {
        "id": "zz00MJrHjVNX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NGjDtbUEikrd"
      },
      "outputs": [],
      "source": [
        "def load_transcriptions(filename):\n",
        "    # Check if the file exists\n",
        "    if not os.path.exists(filename):\n",
        "        # If the file does not exist, return an empty dictionary\n",
        "        return {}\n",
        "    with open(filename, 'r') as f:\n",
        "        return json.load(f)\n",
        "\n",
        "# Load Wav2Vec2 transcriptions\n",
        "wav2vec2bert_transcriptions = load_transcriptions(\"/content/drive/MyDrive/STT/wav2vec2bert_transcriptions20240828-095455.json\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(wav2vec2bert_transcriptions)"
      ],
      "metadata": {
        "id": "vU2-MMdri7gl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transcriptions_dict = {\n",
        "\n",
        "    \"Wav2Vec2-BERT\": wav2vec2bert_transcriptions\n",
        "}"
      ],
      "metadata": {
        "id": "2jBcH9cmjA46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(transcriptions_dict)"
      ],
      "metadata": {
        "id": "FyjELdRKjDf6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accuracy Metrics: add the validation data"
      ],
      "metadata": {
        "id": "vJbm6qPvjHwV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_metric\n",
        "import pandas as pd\n",
        "from google.colab import files\n",
        "\n",
        "# Load the TSV file with reference transcriptions\n",
        "uploaded = files.upload()\n",
        "file_paths = list(uploaded.keys())\n",
        "data = pd.read_csv(file_paths[0], delimiter='\\t')\n",
        "reference_transcriptions = dict(zip(data['path'], data['sentence']))"
      ],
      "metadata": {
        "id": "DFg63yRwjJEu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(reference_transcriptions)"
      ],
      "metadata": {
        "id": "aHL6bb44jQfS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(set(transcriptions_dict['Wav2Vec2-BERT'].keys()))\n",
        "print(set(reference_transcriptions.keys()))\n"
      ],
      "metadata": {
        "id": "6vBg3db5jSpY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(transcriptions_dict)"
      ],
      "metadata": {
        "id": "XW2aXQFRjZla"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize metrics\n",
        "wer_metric = load_metric(\"wer\")\n",
        "bleu_metric = load_metric(\"bleu\")\n",
        "cer_metric = load_metric(\"cer\")"
      ],
      "metadata": {
        "id": "zhiCXgOWjbtL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_transcriptions(transcriptions, reference_transcriptions):\n",
        "    wer_scores, bleu_scores, cer_scores = {}, {}, {}\n",
        "    for sentence_id, predicted_transcription in transcriptions.items():\n",
        "        reference_transcription = reference_transcriptions.get(sentence_id)\n",
        "        if reference_transcription:\n",
        "          print(f\"Evaluating: {sentence_id}\")\n",
        "          print(f\"Predicted: {predicted_transcription}\")\n",
        "          print(f\"Reference: {reference_transcription}\")\n",
        "          wer = wer_metric.compute(predictions=[predicted_transcription], references=[reference_transcription])\n",
        "          bleu = bleu_metric.compute(predictions=[predicted_transcription.split()], references=[[reference_transcription.split()]])\n",
        "          cer = cer_metric.compute(predictions=[predicted_transcription], references=[reference_transcription])\n",
        "\n",
        "          wer_scores[sentence_id] = wer\n",
        "          bleu_scores[sentence_id] = bleu['bleu']\n",
        "          cer_scores[sentence_id] = cer\n",
        "    return wer_scores, bleu_scores, cer_scores"
      ],
      "metadata": {
        "id": "51AuwgsCjfDk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate all models\n",
        "results = {}\n",
        "\n",
        "for model_name, transcriptions in transcriptions_dict.items():\n",
        "  print(f\"Model: {model_name}, Transcriptions: {transcriptions}\")\n",
        "  wer_scores, bleu_scores, cer_scores = evaluate_transcriptions(transcriptions, reference_transcriptions)\n",
        "  results[model_name] = {\n",
        "      \"WER\": wer_scores,\n",
        "      \"BLEU\": bleu_scores,\n",
        "      \"CER\": cer_scores\n",
        "  }"
      ],
      "metadata": {
        "id": "O6Ez0dOrjh_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Reference Transcriptions Keys: {set(reference_transcriptions.keys())}\")\n"
      ],
      "metadata": {
        "id": "U7Wgmn0bjkeb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transcription_ids = set(transcriptions_dict['Wav2Vec2-BERT'].keys())\n",
        "reference_ids = set(reference_transcriptions.keys())\n",
        "\n",
        "print(f\"Transcription IDs: {transcription_ids}\")\n",
        "print(f\"Reference IDs: {reference_ids}\")\n",
        "print(f\"Common IDs: {transcription_ids.intersection(reference_ids)}\")"
      ],
      "metadata": {
        "id": "Jae0HSzujl-C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def create_comparison_table(results):\n",
        "  records = []\n",
        "  for model_name, metrics in results.items():\n",
        "    for sentence_id in metrics[\"WER\"].keys():\n",
        "      records.append({\n",
        "          \"Model\": model_name,\n",
        "          \"Sentence ID\": sentence_id,\n",
        "          \"WER\": metrics[\"WER\"].get(sentence_id, None),\n",
        "          \"BLEU\": metrics[\"BLEU\"].get(sentence_id, None),\n",
        "          \"CER\": metrics[\"CER\"].get(sentence_id, None),\n",
        "        })\n",
        "    df = pd.DataFrame(records)\n",
        "  return df\n",
        "\n",
        "# Create and display the comparison table\n",
        "print(results)\n",
        "comparison_df = create_comparison_table(results)\n",
        "print(comparison_df)\n",
        "\n",
        "# Optionally, save the comparison to a CSV file\n",
        "comparison_df.to_csv('model_comparison.csv', index=False)\n"
      ],
      "metadata": {
        "id": "7JaLL9pMjnm7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}